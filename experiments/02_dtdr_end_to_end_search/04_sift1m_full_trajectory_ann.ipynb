{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# SIFT1M Trajectory Router — v2 (Filtered GT Recall)\n",
    "\n",
    "**Fix applied:** Ground truth is filtered to only count neighbours that exist\n",
    "within BASE_LIMIT. Queries where fewer than 3 valid GT neighbours exist are\n",
    "skipped entirely. This gives a fair evaluation of routing quality on the\n",
    "partial index, rather than being penalised for neighbours that were never loaded.\n",
    "\n",
    "**Other fixes retained from v1:**\n",
    "- L2-consistent node scoring: `2·q·mean - ||mean||²`\n",
    "- Precomputed node squared norms\n",
    "- Increased candidate budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config OK\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "import numpy as np\n",
    "\n",
    "SIFT_DIR  = r\"G:\\train_jw\\datasets\\sift1m\"\n",
    "BASE_NPY  = os.path.join(SIFT_DIR, \"sift_base.npy\")\n",
    "QUERY_NPY = os.path.join(SIFT_DIR, \"sift_query.npy\")\n",
    "GT_IVEC   = os.path.join(SIFT_DIR, \"sift_groundtruth.ivecs\")\n",
    "\n",
    "assert os.path.exists(BASE_NPY),  BASE_NPY\n",
    "assert os.path.exists(QUERY_NPY), QUERY_NPY\n",
    "assert os.path.exists(GT_IVEC),   GT_IVEC\n",
    "\n",
    "BASE_LIMIT  = 200_000\n",
    "QUERY_LIMIT = 2_000\n",
    "TOPK_EVAL   = 10\n",
    "MIN_VALID_GT = 3   # skip query if fewer than this many GT neighbours are in-index\n",
    "\n",
    "NLIST        = 256\n",
    "NPROBE       = 8\n",
    "KMEANS_TRAIN = 50_000\n",
    "\n",
    "BAG_SIZE    = 32\n",
    "assert (BAG_SIZE & (BAG_SIZE-1)) == 0\n",
    "TREE_LEVELS = int(np.log2(BAG_SIZE))  # 5\n",
    "\n",
    "TOP_BAGS_PER_LIST = 32\n",
    "BEAM              = 2\n",
    "CANDS_PER_LIST    = 128\n",
    "FINAL_RERANK      = 500\n",
    "\n",
    "SEED = 0\n",
    "rng  = np.random.default_rng(SEED)\n",
    "\n",
    "def read_ivecs(fname):\n",
    "    a = np.fromfile(fname, dtype=np.int32)\n",
    "    d = a[0]\n",
    "    return a.reshape(-1, d + 1)[:, 1:]\n",
    "\n",
    "print(\"Config OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SIFT...\n",
      "X: (200000, 128) Q: (2000, 128) GT: (2000, 100)\n",
      "Usable queries (>= 3 valid GT neighbours in index): 1995/2000\n",
      "Mean valid GT neighbours per query: 21.7 / 10\n",
      "Expected recall ceiling (if routing perfect): ~2.17\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading SIFT...\")\n",
    "X  = np.load(BASE_NPY,  mmap_mode=\"r\")[:BASE_LIMIT].astype(np.float32)\n",
    "Q  = np.load(QUERY_NPY, mmap_mode=\"r\")[:QUERY_LIMIT].astype(np.float32)\n",
    "GT = read_ivecs(GT_IVEC)[:QUERY_LIMIT, :100]\n",
    "\n",
    "n, d = X.shape\n",
    "print(\"X:\", X.shape, \"Q:\", Q.shape, \"GT:\", GT.shape)\n",
    "\n",
    "# --- Filter GT per query: keep only indices < BASE_LIMIT ---\n",
    "# For each query store the valid GT neighbours and whether the query is usable\n",
    "GT_valid = []\n",
    "query_usable = []\n",
    "for i in range(len(Q)):\n",
    "    valid = [int(g) for g in GT[i] if g < BASE_LIMIT]\n",
    "    GT_valid.append(valid)\n",
    "    query_usable.append(len(valid) >= MIN_VALID_GT)\n",
    "\n",
    "n_usable = sum(query_usable)\n",
    "mean_valid = np.mean([len(v) for v in GT_valid])\n",
    "print(f\"Usable queries (>= {MIN_VALID_GT} valid GT neighbours in index): {n_usable}/{len(Q)}\")\n",
    "print(f\"Mean valid GT neighbours per query: {mean_valid:.1f} / {TOPK_EVAL}\")\n",
    "print(f\"Expected recall ceiling (if routing perfect): ~{mean_valid/TOPK_EVAL:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "kmeans",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training IVF centroids...\n",
      "Centroid train time: 3.99 s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "print(\"Training IVF centroids...\")\n",
    "t0 = time.time()\n",
    "\n",
    "train_idx = rng.choice(n, size=min(KMEANS_TRAIN, n), replace=False)\n",
    "Xtr = np.array(X[train_idx], dtype=np.float32)\n",
    "\n",
    "kmeans = MiniBatchKMeans(\n",
    "    n_clusters=NLIST, batch_size=4096,\n",
    "    n_init=1, max_iter=200,\n",
    "    random_state=SEED, verbose=0\n",
    ")\n",
    "kmeans.fit(Xtr)\n",
    "C    = kmeans.cluster_centers_.astype(np.float32)\n",
    "C_sq = np.sum(C**2, axis=1)\n",
    "\n",
    "print(\"Centroid train time:\", round(time.time()-t0, 2), \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "assign",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigning vectors to IVF lists...\n",
      "Assign time: 0.41 s\n",
      "List sizes — mean: 781  max: 1855  min: 246\n"
     ]
    }
   ],
   "source": [
    "print(\"Assigning vectors to IVF lists...\")\n",
    "t0 = time.time()\n",
    "\n",
    "assign = np.empty(n, dtype=np.int32)\n",
    "chunk  = 50_000\n",
    "for i in range(0, n, chunk):\n",
    "    Xc   = np.array(X[i:i+chunk], dtype=np.float32)\n",
    "    x2   = np.sum(Xc**2, axis=1, keepdims=True)\n",
    "    dist = x2 + C_sq[None, :] - 2.0 * (Xc @ C.T)\n",
    "    assign[i:i+len(Xc)] = np.argmin(dist, axis=1).astype(np.int32)\n",
    "\n",
    "lists = [[] for _ in range(NLIST)]\n",
    "for i, li in enumerate(assign):\n",
    "    lists[int(li)].append(i)\n",
    "\n",
    "list_sizes = np.array([len(l) for l in lists])\n",
    "print(\"Assign time:\", round(time.time()-t0, 2), \"s\")\n",
    "print(f\"List sizes — mean: {list_sizes.mean():.0f}  max: {list_sizes.max()}  min: {list_sizes.min()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "buildtree",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building trajectory tree index...\n",
      "Tree build time: 0.36 s\n",
      "Total bags: 6134\n"
     ]
    }
   ],
   "source": [
    "print(\"Building trajectory tree index...\")\n",
    "t0 = time.time()\n",
    "\n",
    "traj = []\n",
    "total_bags = 0\n",
    "\n",
    "for li in range(NLIST):\n",
    "    idxs = np.array(lists[li], dtype=np.int32)\n",
    "    nb   = len(idxs) // BAG_SIZE\n",
    "    if nb == 0:\n",
    "        traj.append(None)\n",
    "        continue\n",
    "\n",
    "    idxs       = idxs[:nb*BAG_SIZE]\n",
    "    bag_leaves = idxs.reshape(nb, BAG_SIZE)\n",
    "    V          = np.array(X[idxs], dtype=np.float32).reshape(nb, BAG_SIZE, d)\n",
    "\n",
    "    nodes    = []\n",
    "    nodes_sq = []\n",
    "\n",
    "    for L in range(TREE_LEVELS):\n",
    "        nodesL = 2**L\n",
    "        group  = BAG_SIZE // nodesL\n",
    "        M  = np.empty((nb, nodesL, d), dtype=np.float32)\n",
    "        M2 = np.empty((nb, nodesL),    dtype=np.float32)\n",
    "        for j in range(nodesL):\n",
    "            seg        = V[:, j*group:(j+1)*group, :].mean(axis=1)\n",
    "            M[:, j, :] = seg\n",
    "            M2[:, j]   = np.sum(seg**2, axis=1)\n",
    "        nodes.append(M)\n",
    "        nodes_sq.append(M2)\n",
    "\n",
    "    traj.append({\"bag_leaves\": bag_leaves, \"nodes\": nodes, \"nodes_sq\": nodes_sq})\n",
    "    total_bags += nb\n",
    "\n",
    "print(\"Tree build time:\", round(time.time()-t0, 2), \"s\")\n",
    "print(\"Total bags:\", total_bags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "searchfns",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions defined OK\n"
     ]
    }
   ],
   "source": [
    "def centroid_shortlist(qv, nprobe=NPROBE):\n",
    "    q2   = float(np.dot(qv, qv))\n",
    "    dist = q2 + C_sq - 2.0 * (C @ qv)\n",
    "    return np.argsort(dist)[:nprobe].astype(np.int32)\n",
    "\n",
    "\n",
    "def route_within_list(qv, q2, li,\n",
    "                      top_bags=TOP_BAGS_PER_LIST,\n",
    "                      beam=BEAM,\n",
    "                      cands=CANDS_PER_LIST):\n",
    "    t = traj[li]\n",
    "    if t is None:\n",
    "        return np.empty((0,), dtype=np.int32)\n",
    "\n",
    "    bag_leaves = t[\"bag_leaves\"]\n",
    "    nodes      = t[\"nodes\"]\n",
    "    nodes_sq   = t[\"nodes_sq\"]\n",
    "    nb         = bag_leaves.shape[0]\n",
    "\n",
    "    # Stage A: score all bags by L2-proxy to root mean\n",
    "    # score = 2*q·mean - ||mean||^2  (higher = closer in L2)\n",
    "    root    = nodes[0][:, 0, :]   # (nb, d)\n",
    "    root_sq = nodes_sq[0][:, 0]   # (nb,)\n",
    "    scores  = 2.0 * (root @ qv) - root_sq\n",
    "    bag_idx = np.argsort(-scores)[:min(top_bags, nb)]\n",
    "\n",
    "    # Stage B: beam descent\n",
    "    cand_global = []\n",
    "    for b in bag_idx:\n",
    "        cand_nodes = [0]\n",
    "        for L in range(1, TREE_LEVELS):\n",
    "            next_nodes = []\n",
    "            for parent in cand_nodes:\n",
    "                c0, c1 = parent*2, parent*2+1\n",
    "                s0 = 2.0*float(np.dot(nodes[L][b, c0], qv)) - float(nodes_sq[L][b, c0])\n",
    "                s1 = 2.0*float(np.dot(nodes[L][b, c1], qv)) - float(nodes_sq[L][b, c1])\n",
    "                next_nodes.append((c0, s0))\n",
    "                next_nodes.append((c1, s1))\n",
    "            next_nodes.sort(key=lambda x: -x[1])\n",
    "            cand_nodes = [i for i, _ in next_nodes[:beam]]\n",
    "\n",
    "        group_size = BAG_SIZE // (2**(TREE_LEVELS-1))\n",
    "        for node in cand_nodes:\n",
    "            start = node * group_size\n",
    "            for i in range(start, start + group_size):\n",
    "                cand_global.append(int(bag_leaves[b, i]))\n",
    "\n",
    "    if not cand_global:\n",
    "        return np.empty((0,), dtype=np.int32)\n",
    "\n",
    "    cand_global = np.unique(np.array(cand_global, dtype=np.int32))\n",
    "    if len(cand_global) > cands:\n",
    "        V    = np.array(X[cand_global], dtype=np.float32)\n",
    "        dist = np.sum((V - qv)**2, axis=1)\n",
    "        cand_global = cand_global[np.argsort(dist)[:cands]]\n",
    "\n",
    "    return cand_global\n",
    "\n",
    "\n",
    "def search_query(qv, nprobe=NPROBE, top_bags=TOP_BAGS_PER_LIST, beam=BEAM):\n",
    "    q2          = float(np.dot(qv, qv))\n",
    "    probe_lists = centroid_shortlist(qv, nprobe=nprobe)\n",
    "\n",
    "    all_cands = []\n",
    "    for li in probe_lists:\n",
    "        c = route_within_list(qv, q2, int(li), top_bags=top_bags, beam=beam)\n",
    "        if len(c) > 0:\n",
    "            all_cands.append(c)\n",
    "\n",
    "    if not all_cands:\n",
    "        return np.empty((0,), dtype=np.int32)\n",
    "\n",
    "    all_cands = np.unique(np.concatenate(all_cands))\n",
    "    if len(all_cands) > FINAL_RERANK:\n",
    "        V    = np.array(X[all_cands], dtype=np.float32)\n",
    "        dist = np.sum((V - qv)**2, axis=1)\n",
    "        all_cands = all_cands[np.argsort(dist)[:FINAL_RERANK]]\n",
    "\n",
    "    V    = np.array(X[all_cands], dtype=np.float32)\n",
    "    dist = np.sum((V - qv)**2, axis=1)\n",
    "    return all_cands[np.argsort(dist)[:TOPK_EVAL]]\n",
    "\n",
    "\n",
    "def recall_at_k_filtered(found, valid_gt, k=10):\n",
    "    \"\"\"\n",
    "    Recall against only the GT neighbours that exist in the index.\n",
    "    Denominator = min(k, len(valid_gt)) so it stays in [0,1].\n",
    "    \"\"\"\n",
    "    denom = min(k, len(valid_gt))\n",
    "    if denom == 0:\n",
    "        return None\n",
    "    return len(set(found[:k].tolist()) & set(valid_gt)) / denom\n",
    "\n",
    "\n",
    "print(\"Functions defined OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eval",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running main evaluation...\n",
      "Queries evaluated: 1995  (skipped: 5)\n",
      "Mean recall@10 (filtered GT): 0.903\n",
      "Eval time: 16.48 s\n"
     ]
    }
   ],
   "source": [
    "print(\"Running main evaluation...\")\n",
    "t0 = time.time()\n",
    "\n",
    "recalls  = []\n",
    "skipped  = 0\n",
    "\n",
    "for i in range(len(Q)):\n",
    "    if not query_usable[i]:\n",
    "        skipped += 1\n",
    "        continue\n",
    "    top = search_query(Q[i])\n",
    "    r   = recall_at_k_filtered(top, GT_valid[i], k=TOPK_EVAL)\n",
    "    if r is not None:\n",
    "        recalls.append(r)\n",
    "\n",
    "print(f\"Queries evaluated: {len(recalls)}  (skipped: {skipped})\")\n",
    "print(f\"Mean recall@{TOPK_EVAL} (filtered GT): {np.mean(recalls):.3f}\")\n",
    "print(f\"Eval time: {round(time.time()-t0, 2)} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sweep",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  nprobe   top_bags   beam    recall@10   n_eval   time_s\n",
      "----------------------------------------------------------\n",
      "       4         16      2        0.708     1995      5.4\n",
      "       4         32      2        0.812     1995      8.3\n",
      "       4         64      2        0.820     1995      8.8\n",
      "       8         16      2        0.814     1995     10.7\n",
      "       8         32      2        0.903     1995     16.3\n",
      "       8         64      2        0.908     1995     17.4\n",
      "      16         16      2        0.856     1995     20.9\n",
      "      16         32      2        0.930     1995     31.7\n",
      "      16         64      2        0.934     1995     33.8\n"
     ]
    }
   ],
   "source": [
    "# Sweep over nprobe and top_bags\n",
    "print(f\"{'nprobe':>8} {'top_bags':>10} {'beam':>6} {'recall@10':>12} {'n_eval':>8} {'time_s':>8}\")\n",
    "print(\"-\" * 58)\n",
    "\n",
    "for nprobe in [4, 8, 16]:\n",
    "    for top_bags in [16, 32, 64]:\n",
    "        t0 = time.time()\n",
    "        recalls_sw = []\n",
    "        for i in range(len(Q)):\n",
    "            if not query_usable[i]:\n",
    "                continue\n",
    "            top = search_query(Q[i], nprobe=nprobe, top_bags=top_bags, beam=BEAM)\n",
    "            r   = recall_at_k_filtered(top, GT_valid[i], k=TOPK_EVAL)\n",
    "            if r is not None:\n",
    "                recalls_sw.append(r)\n",
    "        dt = time.time() - t0\n",
    "        print(f\"{nprobe:>8} {top_bags:>10} {BEAM:>6} {np.mean(recalls_sw):>12.3f} {len(recalls_sw):>8} {dt:>8.1f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
